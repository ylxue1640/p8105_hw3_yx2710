---
title: "p8105_hw3_yx2710"
output: github_document
---

```{r setup}
library(tidyverse)
library(p8105.datasets)
library(patchwork)
library(hexbin)
```

Problem 1 

```{r} 
data("instacart")
```

Short description of the instacart dataset:
The instacart dataset has `r nrow(instacart)` rows and `r ncol(instacart)` columns. The variables that the instacart dataset contain are `r names(instacart)`

* "reordered": 1 means this product has been ordered by this user before, 0 otherwise.

* "eval_set":  which evaluation set this order is included (Note that eval_set of this dataset is exclusively from the “train”)

* "order_dow": the day of the week on which the order was placed

* "aisle": the name of the aisle

* "department": the name of the department

Q:How many aisles are there, and which aisles are the most items ordered from?
```{r}
instacart %>% 
  count(aisle, name = "n") %>% 
  arrange(-n)
```
A: There are 134 aisles and fresh vegetables are the most items ordered from.

Q: Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.
```{r}
instacart %>% 
  count(aisle, name = "n") %>%
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n, .desc = TRUE)) %>% 
  ggplot(aes(x = aisle, y = n)) + geom_point() +
  labs(
    title = "Number of items ordered in each aisle",
    x = "Aisle",
    y = "Number of items ordered",
    caption = "Oder number larger than 10,000"
  )+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```
A: The plot is shown here and this plot also shows the answer that fresh vegetables are the most items ordered from.

Q:Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.
```{r}
instacart %>% 
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarise(count = n()) %>% 
  top_n(3, count) %>% 
  knitr::kable()
```

A: The table is shown here.

Q: Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).
```{r}
instacart %>% 
  filter(product_name == c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarise(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = product_name,
    values_from = mean_hour
  ) %>% 
  knitr::kable()
```
A: The table is shown here.

Problem 2 

```{r}
accel_df = read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(week_d = ifelse(day %in% c("Saturday", "Sunday"), "Weekend", "Weekday")) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_count"
  ) %>% 
  select(week, day_id, day, week_d, everything()) %>% 
  mutate(
    day = factor(day),
    minute = as.numeric(minute)
  )
```
The `r ncol(accel_df)` variables that dataset contains are `r names(accel_df)` and there are `r nrow(accel_df)` observations.


Create a total activity variable for each day, and create a table showing these totals. 
```{r}
accel_df %>% 
  mutate(day = forcats::fct_relevel(day, c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) %>% 
  group_by(day) %>% 
  summarize(total_activity = sum(activity_count)) %>% 
  knitr::kable()

```
There is no apparent trend.

24-hour activity time courses for each day 
```{r}
accel_df %>% 
  mutate(day = forcats::fct_relevel(day, c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) %>% 
  ggplot(aes(x = minute, y = activity_count, color = day)) +
  geom_point(alpha = 0.3) +
  labs(
    x = "Minutes",
    y = "Activity Count"
  )+
  scale_x_continuous(
    breaks = c(0, 240, 480, 720, 960, 1200, 1440),
    labels = c("0", "4am", "8am", "12pm", "4pm", "8pm", "12am")
  )+ geom_smooth(se = FALSE)+
    viridis::scale_color_viridis(
    name = "Day", 
    discrete = TRUE
  )
```
The plot tells us that most activities happened between 8am-12pm and 8pm-12am.Also, most activity counts are less than 2500.

Problem 3

```{r}
library(p8105.datasets)
data("ny_noaa")
```

The dataset has `r ncol(ny_noaa)` columns and `r nrow(ny_noaa)` rows. The variables this dataset contains are `r names(ny_noaa)`.
Key variables in ny_noaa:
* prcp: Precipitation (tenths of mm)
* snow: Snowfall (mm)
* snwd: Snow depth (mm)
* tmax: Maximum temperature (tenths of degrees C)
* tmin: Minimum temperature (tenths of degrees C)

The proportion of NA in this dataset is `r sum(is.na(ny_noaa))/(5*nrow(ny_noaa))`` and I think it is somewhat severe.

```{r}
ny_noaa_c = ny_noaa %>% 
  separate(date, into = c("year", "month", "day"), sep="-", remove = TRUE) %>% 
   mutate(
    prcp = prcp/10,
    tmax = as.numeric(tmax)/10,
    tmin = as.numeric(tmin)/10
    ) 

ny_noaa_c%>% 
  group_by(snow) %>% 
  summarise(snow_n = n()) %>% 
  arrange(-snow_n)
```
From snow, the most commonly observed value is 0, which means most days in NY do not snow.

Make a two-panel plot

```{r}
Two_panel_plot = ny_noaa_c %>% 
  filter(month %in% c("01","07")) %>% 
  group_by(id, year, month) %>% 
  summarize(mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  drop_na() %>% 
  ggplot(aes(x = year, y = mean_tmax))+
  geom_point(alpha = .2)+
  geom_smooth()+
  facet_grid(. ~ month)+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))+
  xlab("Year") +
  ylab("Average max temperature")
  
Two_panel_plot
```

From the plot, we can figure out that most of average max temperatures of January locate between -10 degrees C to 10 degrees C, and most of average max temperatures of July concentrate between 20 degrees C and 40 degrees C.
There are some outliers, such as in 1982 and 2005, the mean_tmax is lower than -10 degree C in Jan and in July of 1988, the mean_tmax is lower than 20 degrees C.

Make a two-panel plot showing 

(i) tmax vs tmin for the full dataset

(ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.
```{r}
t_t_plot = ny_noaa_c %>% 
  ggplot(aes(x = tmin, y = tmax)) + 
  geom_hex() +
  scale_fill_viridis_c() +
  xlab("Minimum temperature ") +
  ylab("Maximum temperature")

snow_plot = ny_noaa_c %>% 
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = year, y = snow)) +
  geom_boxplot(fill="slateblue", alpha=0.2) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))+
  xlab("Year") +
  ylab("Snowfall(mm)")

t_t_plot + snow_plot 
```




